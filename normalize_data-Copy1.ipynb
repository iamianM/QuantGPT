{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/forex/all_forex_data_ffill_300.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data_clustered', data_clustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(815635, 299)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clustered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustered = np.load('data_clustered.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongLyrics(Dataset):  \n",
    "    def __init__(self, context):\n",
    "        self.context = context\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.context)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        x = torch.tensor(self.context[item][-max_size:]).long()\n",
    "        return x\n",
    "    \n",
    "dataset = SongLyrics(data_clustered)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((815635, 299), torch.Size([299]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dataset.context).shape, dataset.__getitem__(1000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49,\n",
       "        49, 91,  8, 84, 81, 84, 86, 49, 81, 88, 11, 11, 13, 13, 87, 49, 49, 11,\n",
       "        88, 16, 81, 13, 11, 84, 49, 16, 49, 49, 16, 49, 11, 49, 81, 49, 13, 49,\n",
       "        13, 13, 49, 89, 16, 13, 11, 86, 11, 81, 86, 16, 81, 84, 11, 86, 81, 11,\n",
       "        84, 84, 81, 81, 11, 49, 81, 49, 87, 84, 81, 84, 11, 84, 84, 81, 49, 49,\n",
       "        86,  9, 49, 86, 49, 81, 49, 88, 13, 81, 13, 88, 49, 84, 49, 11, 86, 13,\n",
       "        81, 81, 84, 81, 81, 86, 16, 16, 16, 49, 49, 81, 89, 81, 88, 13, 13, 11,\n",
       "        16, 16, 81, 13, 81, 49, 84, 13, 81, 11, 86, 49, 13, 49, 81, 13, 81, 81,\n",
       "        13, 81, 11, 81, 11, 81, 81, 16, 84, 84, 81, 49, 16, 49, 49, 81, 84, 84,\n",
       "        84, 84, 11, 84, 16, 86, 13, 16, 81, 16, 86, 84, 81, 13, 10, 16, 49, 16,\n",
       "        49, 49, 13, 16, 81, 49, 81, 16, 16, 81, 13, 81, 49, 16, 81, 13, 49, 81,\n",
       "        81, 49, 49, 86, 49, 16, 16, 13, 16, 11, 49, 84, 11, 11, 49, 86, 81, 16,\n",
       "        16, 11, 81, 81, 13, 16, 49, 49, 49, 49, 81, 16, 49, 84, 86, 13, 49, 16,\n",
       "        81, 16, 81, 16, 49, 49, 49, 49, 81, 86, 49, 84, 84, 49, 81, 16, 16, 84,\n",
       "        49, 81, 11, 86, 49, 49, 81, 16, 86, 84, 16, 13, 13, 16, 86, 13, 16, 49,\n",
       "        81, 49, 49, 49, 84, 49, 81, 16, 81, 16, 16, 81, 84, 16, 81, 49, 13, 11,\n",
       "        81, 87, 88, 86, 81, 16, 16, 16, 81, 81, 84])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "del model\n",
    "del input_tensor\n",
    "del outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-83528bb0e68e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mattn_pdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGPT2LMHeadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         \"\"\"\n\u001b[1;32m--> 637\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    550\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         \"\"\"\n\u001b[1;32m--> 637\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab_size = len(clusters)\n",
    "# bos_token_id=vocab_size-4\n",
    "# eos_token_id=vocab_size-3\n",
    "# pad_token_id=vocab_size-2\n",
    "# unk_token_id=vocab_size-1\n",
    "    \n",
    "config = transformers.GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "#     bos_token_id=bos_token_id,\n",
    "#     eos_token_id=eos_token_id,\n",
    "#     pad_token_id=pad_token_id,\n",
    "#     unk_token_id=unk_token_id,\n",
    "    bos_token_id=None,\n",
    "    eos_token_id=None,\n",
    "    pad_token_id=None,\n",
    "    unk_token_id=None,\n",
    "    n_positions=max_size,\n",
    "    n_ctx=max_size,\n",
    "    n_embd=12*64,\n",
    "    num_labels=vocab_size,\n",
    "    resid_pdrop=0,\n",
    "    embd_pdrop=0,\n",
    "    attn_pdrop=0,\n",
    ")\n",
    "model = transformers.GPT2LMHeadModel(config).cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3114e53af105>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnum_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGPT2LMHeadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT2Model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Model parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36minit_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_init_weights\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m             \u001b[1;31m# Initialize weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 802\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m             \u001b[1;31m# Tie weights should be skipped when not initializing all weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[0;32m    615\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[0;32m    615\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[0;32m    615\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[0;32m    615\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36m_init_weights\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;31m# Slightly different from the TF version which uses truncated_normal for initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# cf https://github.com/pytorch/pytorch/pull/5617\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "    \n",
    "config = transformers.GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=512*2,\n",
    "    n_ctx=512*2,\n",
    "    n_embd=12*64*8,\n",
    "    num_labels=vocab_size\n",
    ")\n",
    "model = transformers.GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "get_n_params(model)/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_params(model)/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_params(model)/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85361664, 1.6094379124341003, 1.0497581446691984, 2.8569600634776755)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_params(model), np.log(5), np.log(data_clustered.size/get_n_params(model)), data_clustered.size/get_n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                           | 0/18538 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18538/18538 [3:43:13<00:00,  1.38it/s, ACC=0.105, ACC_sign=0.228, a_mae=0.389, loss=3.1560032, lr=3e-11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.15600323677063, ACC: 0.13975646668482064, ACC_sign: 0.2496758507995122, MAE: 0.3669678239079964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=1\n",
    "warmup_steps=2000//(512/64)\n",
    "# warmup_steps = 100\n",
    "lr=0.000025\n",
    "output_dir=\"./runs\"\n",
    "output_prefix=\"first\"\n",
    "save_model_on_epoch=True\n",
    "time_steps = 30\n",
    "    \n",
    "device=torch.device(\"cuda\")\n",
    "\n",
    "batch_size_loader=44\n",
    "batch_size = 512//batch_size_loader\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size_loader, shuffle=True)\n",
    "loss=0\n",
    "accumulating_batch_count = 0\n",
    "input_tensor = None\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0)\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs*train_dataloader.__len__()//batch_size\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f\"Training epoch {epoch}\")\n",
    "        \n",
    "    pbar = tqdm(enumerate(train_dataloader), total=train_dataloader.__len__())\n",
    "    acc = []\n",
    "    acc_sign = []\n",
    "    mae = []\n",
    "    samples = np.empty((0, 30))\n",
    "    input_tensors = torch.empty((0, max_size), dtype=torch.long).cuda()\n",
    "    for idx, input_tensor in pbar:\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        \n",
    "        outputs = model(input_tensor, labels=input_tensor)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        \n",
    "        inp = input_tensor.cpu().detach().numpy()\n",
    "        logits = outputs['logits'].argmax(axis=2).cpu().detach().numpy()\n",
    "        \n",
    "        acc.append((logits == inp).sum()/logits.size)\n",
    "        acc_sign.append((np.sign(logits-len(clusters)//2) == np.sign(inp-len(clusters)//2)).sum()/logits.size)\n",
    "        mae.append(mean_absolute_error(inp, logits)/len(clusters))\n",
    "\n",
    "        if (accumulating_batch_count % batch_size) == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            \n",
    "        accumulating_batch_count += 1\n",
    "        \n",
    "        input_tensor = None\n",
    "        pbar.set_postfix(loss=loss.cpu().detach().numpy(), ACC=acc[-1], ACC_sign=acc_sign[-1], a_mae=mae[-1],\n",
    "                         lr=scheduler.get_last_lr()[0])\n",
    "        \n",
    "    if save_model_on_epoch:\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "        )\n",
    "        \n",
    "    print(f\"Loss: {loss}, ACC: {np.mean(acc)}, ACC_sign: {np.mean(acc_sign)}, MAE: {np.mean(mae)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c0457b9ba8>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                           | 0/25489 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "  0%|                                                                                                                              | 16/25489 [00:13<5:50:00,  1.21it/s, ACC=0.0115, ACC_sign=0.434, a_mae=0.387, loss=4.7863426, lr=1e-7]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.28 GiB (GPU 0; 24.00 GiB total capacity; 16.35 GiB already allocated; 2.73 GiB free; 19.05 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a0742f539a7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m             output = model.generate(\n\u001b[0;32m     48\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             )\n\u001b[0;32m     51\u001b[0m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m             )\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1792\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1794\u001b[1;33m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1795\u001b[0m             )\n\u001b[0;32m   1796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    957\u001b[0m         )\n\u001b[0;32m    958\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    797\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m                 )\n\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         )\n\u001b[0;32m    325\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[0mpresent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_attn_weights\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_cross_attention\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.28 GiB (GPU 0; 24.00 GiB total capacity; 16.35 GiB already allocated; 2.73 GiB free; 19.05 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "epochs=1\n",
    "warmup_steps=2000//(512/64)\n",
    "# warmup_steps = 100\n",
    "lr=0.000025\n",
    "output_dir=\"./runs\"\n",
    "output_prefix=\"first\"\n",
    "save_model_on_epoch=True\n",
    "time_steps = 30\n",
    "    \n",
    "device=torch.device(\"cuda\")\n",
    "\n",
    "batch_size_loader=32\n",
    "batch_size = 512//batch_size_loader\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size_loader, shuffle=True)\n",
    "loss=0\n",
    "accumulating_batch_count = 0\n",
    "input_tensor = None\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0)\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs*train_dataloader.__len__()//batch_size\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print(f\"Training epoch {epoch}\")\n",
    "        \n",
    "    pbar = tqdm(enumerate(train_dataloader), total=train_dataloader.__len__())\n",
    "#     mae_profit = []\n",
    "    acc = []\n",
    "    acc_sign = []\n",
    "    mae = []\n",
    "#     mae_cash = []\n",
    "    samples = np.empty((0, 30))\n",
    "#     cash_preds = np.empty((0, 30))\n",
    "#     cash_trues = np.empty((0, 30))\n",
    "#     profit_preds = np.empty((0, 30))\n",
    "#     profit_trues = np.empty((0, 30))\n",
    "    input_tensors = torch.empty((0, max_size), dtype=torch.long).cuda()\n",
    "    for idx, input_tensor in pbar:\n",
    "        input_tensor = input_tensor.to(device)\n",
    "#         input_tensors = torch.cat((input_tensors, input_tensor))\n",
    "        \n",
    "        if idx == 0 or (accumulating_batch_count % batch_size) == 0:\n",
    "            output = model.generate(\n",
    "                input_ids=input_tensor[:, :-time_steps],\n",
    "                max_length=max_size, temperature=1.0, do_sample=False, top_k=100, seed=42, num_beams=3\n",
    "            )\n",
    "            sample = output[:,-time_steps:].cpu().detach().numpy()\n",
    "            samples = np.concatenate((samples, sample))\n",
    "\n",
    "            inp = input_tensors[:,-time_steps:].cpu().detach().numpy()\n",
    "            acc.append((sample == inp).sum()/sample.size)\n",
    "            acc_sign.append((np.sign(sample-len(clusters)//2) == np.sign(inp-len(clusters)//2)).sum()/sample.size)\n",
    "            mae.append(mean_absolute_error(inp, sample)/len(clusters))\n",
    "            \n",
    "\n",
    "#             profit_pred = clusters[sample]\n",
    "#             profit_true = clusters[inp]\n",
    "\n",
    "#             profit_pred = np.sign(profit_pred)*(np.exp(abs(profit_pred)) - 1)\n",
    "#             profit_true = np.sign(profit_true)*(np.exp(abs(profit_true)) - 1)\n",
    "\n",
    "#             profit_preds = np.concatenate((profit_preds, profit_pred))\n",
    "#             profit_trues = np.concatenate((profit_trues, profit_true))\n",
    "\n",
    "#             mae_profit.append(mean_absolute_error(profit_true, profit_pred))\n",
    "\n",
    "#             cash_pred = np.ones(profit_pred.shape)\n",
    "#             cash_true = np.ones(profit_true.shape)\n",
    "#             for i in range(1, 30):\n",
    "#                 cash_pred[:,i] = cash_pred[:,i-1]*(1+profit_pred[:,i]/100)\n",
    "#                 cash_true[:,i] = cash_true[:,i-1]*(1+profit_true[:,i]/100)\n",
    "\n",
    "#             cash_preds = np.concatenate((cash_preds, cash_pred))\n",
    "#             cash_trues = np.concatenate((cash_trues, cash_true))\n",
    "\n",
    "#             mae_cash.append(mean_absolute_error(cash_true-1, cash_pred-1))\n",
    "        \n",
    "        outputs = model(input_tensor, labels=input_tensor)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "\n",
    "        if (accumulating_batch_count % batch_size) == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            \n",
    "        accumulating_batch_count += 1\n",
    "        \n",
    "        input_tensor = None\n",
    "#         pbar.set_postfix(loss=loss.cpu().detach().numpy(), ACC= acc[-1], mae_profit=mae_profit[-1], mae_cash=mae_cash[-1], \n",
    "#                          lr=scheduler.get_last_lr()[0])\n",
    "        pbar.set_postfix(loss=loss.cpu().detach().numpy(), ACC= acc[-1], ACC_sign= acc_sign[-1], a_mae=mae[-1],\n",
    "                         lr=scheduler.get_last_lr()[0])\n",
    "        \n",
    "    if save_model_on_epoch:\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "        )\n",
    "        \n",
    "    print(f\"Loss: {loss}, ACC: {np.mean(acc)}, ACC_sign: {np.mean(acc_sign)}, MAE: {np.mean(mae)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(output_dir, f\"one_epoch_all_forex.pt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-ca8f6b96e802>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# del output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# del input_tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'input_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "# del output\n",
    "# del input_tensors\n",
    "del input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del sample\n",
    "del samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cash_preds \n",
    "del cash_trues \n",
    "del profit_preds \n",
    "del profit_trues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_test\n",
    "del test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.load('data/forex/all_forex_data_ffill_test.csv.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_clustered = np.digitize(data_test, clusters, right=False)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data_test_clustered', data_test_clustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_clustered = np.load('data_test_clustered.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = SongLyrics(data_test_clustered) \n",
    "test_dataloader = DataLoader(dataset_test, batch_size=56, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                             | 0/332 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 134.00 MiB (GPU 0; 24.00 GiB total capacity; 20.65 GiB already allocated; 0 bytes free; 22.18 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-580b15b87b8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     output = model.generate(\n\u001b[0;32m     15\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1061\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m             )\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1792\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1794\u001b[1;33m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1795\u001b[0m             )\n\u001b[0;32m   1796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    957\u001b[0m         )\n\u001b[0;32m    958\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 733\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    734\u001b[0m         \u001b[0mposition_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    158\u001b[0m         return F.embedding(\n\u001b[0;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2041\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2043\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 0; 24.00 GiB total capacity; 20.65 GiB already allocated; 0 bytes free; 22.18 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "time_steps = 30\n",
    "pbar = tqdm(enumerate(test_dataloader), total=test_dataloader.__len__())\n",
    "mae_profit = []\n",
    "acc = []\n",
    "mae_cash = []\n",
    "samples = np.empty((0, 30))\n",
    "cash_preds = np.empty((0, 30))\n",
    "cash_trues = np.empty((0, 30))\n",
    "profit_preds = np.empty((0, 30))\n",
    "profit_trues = np.empty((0, 30))\n",
    "for idx, input_tensor in pbar:\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids=input_tensor[:, :-time_steps],\n",
    "        max_length=max_size, temperature=0.5, do_sample=False, top_k=100, seed=42, num_beams=3, repetition_penalty=0.5\n",
    "    )\n",
    "    sample = output[:,-time_steps:].cpu().detach().numpy()\n",
    "    samples = np.concatenate((samples, sample))\n",
    "    \n",
    "    inp = input_tensor[:,-time_steps:].cpu().detach().numpy()\n",
    "    acc.append((sample == inp).sum()/sample.size)\n",
    "\n",
    "    profit_pred = clusters[sample]\n",
    "    profit_true = clusters[inp]\n",
    "\n",
    "    profit_pred = np.sign(profit_pred)*(np.exp(abs(profit_pred)) - 1)\n",
    "    profit_true = np.sign(profit_true)*(np.exp(abs(profit_true)) - 1)\n",
    "    \n",
    "    profit_preds = np.concatenate((profit_preds, profit_pred))\n",
    "    profit_trues = np.concatenate((profit_trues, profit_true))\n",
    "\n",
    "    mae_profit.append(mean_absolute_error(profit_true, profit_pred))\n",
    "\n",
    "    cash_pred = np.ones(profit_pred.shape)\n",
    "    cash_true = np.ones(profit_true.shape)\n",
    "    for i in range(1, 30):\n",
    "        cash_pred[:,i] = cash_pred[:,i-1]*(1+profit_pred[:,i]/100)\n",
    "        cash_true[:,i] = cash_true[:,i-1]*(1+profit_true[:,i]/100)\n",
    "        \n",
    "    cash_preds = np.concatenate((cash_preds, cash_pred))\n",
    "    cash_trues = np.concatenate((cash_trues, cash_true))\n",
    "\n",
    "    mae_cash.append(mean_absolute_error(cash_true-1, cash_pred-1))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70, 70, 18, ..., 17, 20, 19],\n",
       "       [17, 16, 82, ..., 81, 82, 11],\n",
       "       [18, 24, 80, ..., 78, 81, 19],\n",
       "       ...,\n",
       "       [76, 78, 27, ..., 76, 80, 23],\n",
       "       [20, 15, 18, ..., 21, 23, 79],\n",
       "       [86, 49, 49, ..., 70, 49, 74]], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06369047619047619, 0.5913095238095238)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inp == 49).sum()/inp.size, (samples == 49).sum()/samples.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06190476190476191, 0.566537863252662, 0.018833705766187547)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[-1], mae_profit[-1], mae_cash[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005327927264014617\n",
      "0.006650414533385384\n",
      "0.008242009673536535\n",
      "0.008732877882193854\n",
      "0.010007780227318125\n",
      "0.011146968004746757\n",
      "0.012357496258328058\n",
      "0.012808061501812984\n",
      "0.014038753274218151\n",
      "0.014923184883101044\n",
      "0.015632607866635964\n",
      "0.016322336159430607\n",
      "0.017123284305053343\n",
      "0.01825820379885913\n",
      "0.018967354200362576\n",
      "0.019197868810065116\n",
      "0.020285440034510598\n",
      "0.02023784302688332\n",
      "0.020653531918031993\n",
      "0.021012097491917528\n",
      "0.021937939418352228\n",
      "0.02218103836111131\n",
      "0.023333730137836454\n",
      "0.023147987172615122\n",
      "0.023595093378702997\n",
      "0.023786638446498677\n",
      "0.024526074586196434\n",
      "0.025315204123370603\n",
      "0.0254454107844012\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 30):\n",
    "    print(mean_absolute_error(cash_trues[:,i]-1, cash_preds[:,i]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995249003244331 0.987171967456663 1.000475099675567 -1.282803254333698 0.04750996755668879\n",
      "1.0123468382736707 0.9828171817154095 0.987171967456663 -1.718281828459045 -1.282803254333698\n",
      "1.0170730008791997 0.987171967456663 0.9953314788698423 -1.282803254333698 -0.4668521130157741\n",
      "1.017661330120389 0.987171967456663 0.9994215466926359 -1.282803254333698 -0.05784533073640663\n",
      "1.0253423955969467 1.012828032543337 1.0075477619608963 1.282803254333698 0.7547761960896255\n",
      "1.0240217818563366 0.987171967456663 1.001287973408962 -1.282803254333698 0.12879734089618688\n",
      "1.0162927092041123 1.012828032543337 0.9924522380391038 1.282803254333698 -0.7547761960896255\n",
      "0.9988299362579041 0.9828171817154095 1.0171828182845903 -1.718281828459045 1.718281828459045\n",
      "0.9860169133303285 1.012828032543337 0.987171967456663 1.282803254333698 -1.282803254333698\n",
      "1.0029594627776162 1.012828032543337 1.0171828182845903 1.282803254333698 1.718281828459045\n",
      "1.0023792975592203 0.9828171817154095 1.000578453307364 -1.718281828459045 0.05784533073640663\n",
      "1.026021531420097 0.9764138845261018 0.9764138845261018 -2.358611547389827 -2.358611547389827\n",
      "1.027643661902563 0.987171967456663 0.9984190093162852 -1.282803254333698 -0.15809906837147603\n",
      "1.0245787344551767 0.9828171817154095 1.00298248075769 -1.718281828459045 0.2982480757689816\n",
      "1.0323120108530408 0.987171967456663 0.9924522380391038 -1.282803254333698 -0.7547761960896255\n",
      "1.0303029277108156 0.9828171817154095 1.0019461975847448 -1.718281828459045 0.19461975847447333\n",
      "0.9958941670344088 0.9828171817154095 1.033396741629045 -1.718281828459045 3.3396741629044966\n"
     ]
    }
   ],
   "source": [
    "cash = 1.0\n",
    "time_steps = 1\n",
    "threshold = 0.01\n",
    "for i in range(cash_preds.shape[0]):\n",
    "    if cash_preds[i, time_steps] > 1.0 + threshold:\n",
    "        cash = cash*cash_trues[i,time_steps]\n",
    "        print(cash, cash_preds[i,time_steps], cash_trues[i,time_steps], profit_preds[i, time_steps], profit_trues[i, time_steps])\n",
    "    if cash_preds[i, time_steps] < 1.0 - threshold:\n",
    "        cash = cash*(2-cash_trues[i,time_steps])\n",
    "        print(cash, cash_preds[i,time_steps], cash_trues[i,time_steps], profit_preds[i, time_steps], profit_trues[i, time_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99628057, 0.99925196, 0.99553532, 0.99850448,\n",
       "       0.99479062, 0.99775756, 0.99404647, 0.9970112 , 0.99330289,\n",
       "       0.99626539, 0.99255986, 0.99552015, 0.99181738, 0.99477546,\n",
       "       0.99107547, 0.99403133, 0.9903341 , 0.99328775, 0.98959329,\n",
       "       0.99254474, 0.98885304, 0.99180227, 0.98811334, 0.99106037,\n",
       "       0.98737419, 0.99031901, 0.9866356 , 0.98957822, 0.98589755])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cash_preds[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
