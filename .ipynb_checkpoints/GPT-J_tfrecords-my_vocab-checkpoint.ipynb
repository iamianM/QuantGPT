{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "BvAEHzZ5c3_1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer, models\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [1:50:56<00:00, 125.59s/it, file=ZRX_1min.txt]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [24:26<00:00, 27.67s/it, file=ZRX_5min.txt]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [04:40<00:00,  5.30s/it, file=ZRX_30min.txt]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [02:26<00:00,  2.77s/it, file=ZRX_1hour.txt]\n"
     ]
    }
   ],
   "source": [
    "path = 'data/firstratedata/'\n",
    "zipfiles = [f'crypto-active_1min_1nfzd.zip', f'crypto-active_5min_ziyhm.zip', \n",
    "            f'crypto-active_30min_wnk5g.zip', \n",
    "            f'crypto-active_1hour_9tsmq.zip']\n",
    "for zipfile in zipfiles:\n",
    "    zf = ZipFile(f'{path+zipfile}') \n",
    "    data_files = zf.filelist\n",
    "    pbar = tqdm(enumerate(data_files), total=len(data_files))\n",
    "    for idx, file in pbar:\n",
    "        if file.filename.startswith('USDC') or file.filename.startswith('USDT') or file.filename.startswith('BTC-EUR') or file.filename.startswith('ETH-BTC') or file.filename.startswith('UST'):\n",
    "            continue\n",
    "        pbar.set_postfix(file=file.filename)\n",
    "        df = pd.read_csv(zf.open(file), names=['datetime', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        df['datetime'] = df['datetime'].apply(lambda x: x.replace(' ', 'T').replace('-', '').replace(':', '')[:-2])\n",
    "        df = df.round(6)\n",
    "        \n",
    "        df_train = df[~df['datetime'].str.contains('2021')]\n",
    "        df_val = df[df['datetime'].str.contains('2021')]\n",
    "        \n",
    "        tokens = ';'.join([f\"{row['datetime']},{row['open']},{row['high']},{row['low']},{row['close']},{int(row['volume'])}\" for idx, row in df_train.iterrows()])\n",
    "        text_file = open(f\"strings/train/{file.filename[:-4]}_string.txt\", \"w\")\n",
    "        n = text_file.write(tokens)\n",
    "        text_file.close()  \n",
    "        \n",
    "        tokens = ';'.join([f\"{row['datetime']},{row['open']},{row['high']},{row['low']},{row['close']},{int(row['volume'])}\" for idx, row in df_val.iterrows()])\n",
    "        text_file = open(f\"strings/val/{file.filename[:-4]}_string.txt\", \"w\")\n",
    "        n = text_file.write(tokens)\n",
    "        text_file.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE()).from_file('tokenizer/byte-level-bpe.tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in listdir('strings/train/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADA_1hour_string.txt\n",
      "ADA_1min_string.txt\n",
      "ADA_30min_string.txt\n",
      "ADA_5min_string.txt\n",
      "BAT_1hour_string.txt\n",
      "BAT_1min_string.txt\n",
      "BAT_30min_string.txt\n",
      "BAT_5min_string.txt\n",
      "BCH_1hour_string.txt\n",
      "BCH_1min_string.txt\n",
      "BCH_30min_string.txt\n",
      "BCH_5min_string.txt\n",
      "BNT_1hour_string.txt\n",
      "BNT_1min_string.txt\n",
      "BNT_30min_string.txt\n",
      "BNT_5min_string.txt\n",
      "BSV_1hour_string.txt\n",
      "BSV_1min_string.txt\n",
      "BSV_30min_string.txt\n",
      "BSV_5min_string.txt\n",
      "BTC_1hour_string.txt\n"
     ]
    }
   ],
   "source": [
    "samples_tokenized = []\n",
    "for file in files:\n",
    "    print(file)\n",
    "    f = Path('strings/train/'+file).read_text()\n",
    "    tokens = tokenizer.encode(f)\n",
    "    samples_tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"\n",
    "    Returns an int64_list from a bool / enum / int / uint.\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def write_to_file(writer, data):\n",
    "    \"\"\"\n",
    "    writes data to tfrecord file\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        \"text\": _int64_feature(data)\n",
    "    }\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "    \n",
    "    \n",
    "def split_list(l, n):\n",
    "    # splits list/string into n size chunks\n",
    "    return [l[i:i + n] for i in range(0, len(l), n)]\n",
    "\n",
    "\n",
    "def write_files(files, files_per, output_dir, out_name, start_no, write_remainder=True, process_no=None):\n",
    "    # writes a list of files to .tfrecords\n",
    "    chunks = split_list(files, files_per)\n",
    "\n",
    "    if len(chunks[-1]) != files_per and not write_remainder:  # pop the last file if it's length != files per\n",
    "        remainder = chunks.pop(-1)\n",
    "    else:\n",
    "        remainder = None  # assuming files = remainder from an old chunk here\n",
    "        files_per = len(chunks[-1])\n",
    "\n",
    "    for files in chunks:\n",
    "        fp = f\"{output_dir}/{out_name}_{start_no}\"\n",
    "        if process_no is not None:\n",
    "            fp += f\"_{process_no}\"\n",
    "        fp += f\"_{files_per}\"  # add number of files in tfrecord to end of fp\n",
    "        fp += \".tfrecords\"\n",
    "        with tf.io.TFRecordWriter(fp) as writer:\n",
    "            for f in files:\n",
    "                write_to_file(writer, f)\n",
    "        start_no += 1\n",
    "    return start_no, remainder\n",
    "\n",
    "\n",
    "def create_tfrecords(tokenized_files_array, output_dir, out_name):\n",
    "    files_per = 100000\n",
    "    _tfrecord_count, remainder = write_files(tokenized_files_array, files_per=files_per,\n",
    "                                                         output_dir=output_dir, out_name=out_name,\n",
    "                                                         start_no=0, process_no=None)\n",
    "    \n",
    "@tf.function\n",
    "def decode_fn(record_bytes):\n",
    "  return tf.io.parse_single_example(\n",
    "      # Data\n",
    "      record_bytes,\n",
    "\n",
    "      # Schema\n",
    "      {\"x\": tf.io.FixedLenFeature([], dtype=tf.float32),\n",
    "       \"y\": tf.io.FixedLenFeature([], dtype=tf.float32)}\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = create_tfrecords(samples_tokenized, 'records_scratch', 'crypto_train')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GPT2_final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
