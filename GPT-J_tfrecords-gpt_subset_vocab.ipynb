{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BvAEHzZ5c3_1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [2:09:19<00:00, 146.41s/it, file=ZRX_1min.txt]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [30:32<00:00, 34.58s/it, file=ZRX_5min.txt]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [05:50<00:00,  6.62s/it, file=ZRX_30min.txt]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 53/53 [03:03<00:00,  3.46s/it, file=ZRX_1hour.txt]\n"
     ]
    }
   ],
   "source": [
    "path = 'data/firstratedata/'\n",
    "zipfiles = [f'crypto-active_1min_1nfzd.zip', f'crypto-active_5min_ziyhm.zip', \n",
    "            f'crypto-active_30min_wnk5g.zip', \n",
    "            f'crypto-active_1hour_9tsmq.zip']\n",
    "dayOfWeek={0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}\n",
    "samples_tokenized = set([])\n",
    "for zipfile in zipfiles:\n",
    "    zf = ZipFile(f'{path+zipfile}') \n",
    "    data_files = zf.filelist\n",
    "    pbar = tqdm(enumerate(data_files), total=len(data_files))\n",
    "    for idx, file in pbar:\n",
    "        if file.filename.startswith('USDC') or file.filename.startswith('USDT') or file.filename.startswith('BTC-EUR') or file.filename.startswith('ETH-BTC') or file.filename.startswith('UST'):\n",
    "            continue\n",
    "        pbar.set_postfix(file=file.filename)\n",
    "        df = pd.read_csv(zf.open(file), names=['datetime', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df['weekday'] = df['datetime'].dt.dayofweek.map(dayOfWeek)\n",
    "        grid = (df['datetime'].dt.year == 2021) & (df['datetime'].dt.month == 7)\n",
    "        df = df.round(6)\n",
    "        \n",
    "        df_train = df[~grid]\n",
    "        df_val = df[grid]\n",
    "        \n",
    "        coin = file.filename.split('_')[0]\n",
    "        tokens = ';'.join([f\"crypt{row['datetime'].month}_{row['datetime'].day}{row['weekday']}{row['datetime'].time().isoformat()[:5]}O{row['open']}H{row['high']}L{row['low']}C{row['close']}V{int(row['volume'])}\" for idx, row in df_train.iterrows()])\n",
    "        \n",
    "        text_file = open(f\"strings/train/{file.filename[:-4]}_string.txt\", \"w\")\n",
    "        n = text_file.write(tokens)\n",
    "        text_file.close()  \n",
    "        \n",
    "#         tokens = tokenizer.encode(tokens)\n",
    "#         samples_tokenized.update(set(tokens))\n",
    "        \n",
    "        tokens = ';'.join([f\"crypt{row['datetime'].month}_{row['datetime'].day}{row['weekday']}{row['datetime'].time().isoformat()[:5]}O{row['open']}H{row['high']}L{row['low']}C{row['close']}V{int(row['volume'])}\" for idx, row in df_val.iterrows()])\n",
    "        text_file = open(f\"strings/val/{file.filename[:-4]}_string.txt\", \"w\")\n",
    "        n = text_file.write(tokens)\n",
    "        text_file.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = {}\n",
    "\n",
    "f = open('merges.txt', 'r', encoding=\"utf8\")\n",
    "lines = f.readlines()\n",
    "lines[1:]\n",
    "f.close()\n",
    "with open('gpt_subset_tokenizer/merges.txt', 'w') as filehandle:\n",
    "    for line in lines:\n",
    "        t = line.replace(' ', '').replace('\\n', '')\n",
    "        if t.isdigit():\n",
    "            filehandle.write(line)\n",
    "            words = line.replace('\\n', '').split(' ')\n",
    "            words += [''.join(words)]\n",
    "            for w, ID in zip(words, tokenizer.convert_tokens_to_ids(words)):\n",
    "                new_vocab[w] = ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = [\n",
    "    'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun',\n",
    "    'O', 'H', 'L', 'C', 'V',\n",
    "    'crypt', ';', '.', ':', '_'\n",
    "]\n",
    "\n",
    "for w, ID in zip(new_words, tokenizer.convert_tokens_to_ids(new_words)):  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    new_vocab[w] = ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-14b6eb5eee1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'new_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "len(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(new_vocab, open('gpt_subset_tokenizer/vocab_mine.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer_new = GPT2TokenizerFast('gpt_subset_tokenizer/vocab.json', 'gpt_subset_tokenizer/merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_new = GPT2TokenizerFast.from_pretrained('gpt_subset_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29609"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_new.vocab['crypt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([29609], [])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"crypt\"\n",
    "tokenizer.encode(t), tokenizer_new.encode(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 16]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29609,\n",
       " 22,\n",
       " 62,\n",
       " 16,\n",
       " 39902,\n",
       " 405,\n",
       " 25,\n",
       " 405,\n",
       " 46,\n",
       " 14877,\n",
       " 1899,\n",
       " 13,\n",
       " 15,\n",
       " 39,\n",
       " 14877,\n",
       " 4869,\n",
       " 13,\n",
       " 2682,\n",
       " 43,\n",
       " 2682,\n",
       " 42250,\n",
       " 13,\n",
       " 2857,\n",
       " 34,\n",
       " 2682,\n",
       " 33438,\n",
       " 13,\n",
       " 5999,\n",
       " 53,\n",
       " 1238,\n",
       " 26,\n",
       " 29609,\n",
       " 22,\n",
       " 62,\n",
       " 16,\n",
       " 39902,\n",
       " 405,\n",
       " 25,\n",
       " 486,\n",
       " 46,\n",
       " 2682,\n",
       " 33438,\n",
       " 13,\n",
       " 5999,\n",
       " 39,\n",
       " 14877,\n",
       " 2920,\n",
       " 13,\n",
       " 2718,\n",
       " 43,\n",
       " 27371,\n",
       " 4869,\n",
       " 13,\n",
       " 2857,\n",
       " 34,\n",
       " 2682,\n",
       " 41561,\n",
       " 13,\n",
       " 3553,\n",
       " 53,\n",
       " 1415,\n",
       " 26,\n",
       " 29609,\n",
       " 22,\n",
       " 62,\n",
       " 16,\n",
       " 39902,\n",
       " 405,\n",
       " 25,\n",
       " 2999,\n",
       " 46,\n",
       " 27371,\n",
       " 3459,\n",
       " 13,\n",
       " 1157,\n",
       " 39,\n",
       " 2327,\n",
       " 28645,\n",
       " 13,\n",
       " 4310,\n",
       " 43,\n",
       " 27371,\n",
       " 3132,\n",
       " 13,\n",
       " 2919,\n",
       " 34,\n",
       " 2682,\n",
       " 4846,\n",
       " 16,\n",
       " 13,\n",
       " 17,\n",
       " 53,\n",
       " 2425,\n",
       " 26,\n",
       " 29609,\n",
       " 22,\n",
       " 62,\n",
       " 16,\n",
       " 39902,\n",
       " 405,\n",
       " 25,\n",
       " 3070,\n",
       " 46,\n",
       " 27371,\n",
       " 3134,\n",
       " 13,\n",
       " 24,\n",
       " 39,\n",
       " 2327,\n",
       " 22544,\n",
       " 13,\n",
       " 2996,\n",
       " 43,\n",
       " 2682,\n",
       " 48372,\n",
       " 13,\n",
       " 1485,\n",
       " 34,\n",
       " 2682,\n",
       " 4846,\n",
       " 16,\n",
       " 13,\n",
       " 1954,\n",
       " 53,\n",
       " 1983,\n",
       " 26,\n",
       " 29609,\n",
       " 22,\n",
       " 62,\n",
       " 16,\n",
       " 39902,\n",
       " 405,\n",
       " 25,\n",
       " 3023,\n",
       " 46,\n",
       " 2682,\n",
       " 4846,\n",
       " 17,\n",
       " 13,\n",
       " 21,\n",
       " 39,\n",
       " 2327,\n",
       " 20943,\n",
       " 13,\n",
       " 22,\n",
       " 43,\n",
       " 2682,\n",
       " 4846,\n",
       " 17,\n",
       " 13,\n",
       " 3270,\n",
       " 34,\n",
       " 2682,\n",
       " 4089,\n",
       " 18,\n",
       " 13,\n",
       " 2548,\n",
       " 53,\n",
       " 940,\n",
       " 26,\n",
       " 29609,\n",
       " 22,\n",
       " 62,\n",
       " 16,\n",
       " 39902,\n",
       " 405,\n",
       " 25,\n",
       " 2713,\n",
       " 46,\n",
       " 27371,\n",
       " 4869,\n",
       " 13,\n",
       " 3132,\n",
       " 39,\n",
       " 2682,\n",
       " 4089,\n",
       " 18,\n",
       " 13,\n",
       " 2718,\n",
       " 43,\n",
       " 27371,\n",
       " 3023,\n",
       " 13,\n",
       " 4349,\n",
       " 34,\n",
       " 27371,\n",
       " 2713,\n",
       " 13,\n",
       " 3070,\n",
       " 53,\n",
       " 1485,\n",
       " 26,\n",
       " 29609,\n",
       " 22,\n",
       " 62,\n",
       " 16,\n",
       " 39902,\n",
       " 405,\n",
       " 25,\n",
       " 3312,\n",
       " 46,\n",
       " 27371,\n",
       " 2713,\n",
       " 13,\n",
       " 2713,\n",
       " 39,\n",
       " 27371,\n",
       " 2670,\n",
       " 13,\n",
       " 15,\n",
       " 43,\n",
       " 2682,\n",
       " 12865,\n",
       " 13,\n",
       " 486,\n",
       " 34,\n",
       " 27371,\n",
       " 1270,\n",
       " 13,\n",
       " 23,\n",
       " 53,\n",
       " 1129,\n",
       " 26,\n",
       " 29609,\n",
       " 22,\n",
       " 62,\n",
       " 16,\n",
       " 39902,\n",
       " 405,\n",
       " 25,\n",
       " 2998,\n",
       " 46,\n",
       " 27371,\n",
       " 1065,\n",
       " 13,\n",
       " 2548,\n",
       " 39,\n",
       " 27371,\n",
       " 4304,\n",
       " 13,\n",
       " 15,\n",
       " 43,\n",
       " 27371,\n",
       " 940,\n",
       " 13,\n",
       " 3559,\n",
       " 34,\n",
       " 27371,\n",
       " 2327,\n",
       " 13,\n",
       " 18,\n",
       " 53,\n",
       " 1959,\n",
       " 26,\n",
       " 29609,\n",
       " 22,\n",
       " 62,\n",
       " 16,\n",
       " 39902,\n",
       " 405,\n",
       " 25,\n",
       " 2919,\n",
       " 46,\n",
       " 27371,\n",
       " 2623,\n",
       " 13,\n",
       " 1954,\n",
       " 39,\n",
       " 27371,\n",
       " 3720,\n",
       " 13,\n",
       " 3865,\n",
       " 43,\n",
       " 27371,\n",
       " 1507,\n",
       " 13,\n",
       " 4524,\n",
       " 34,\n",
       " 27371,\n",
       " 3720,\n",
       " 13,\n",
       " 3865,\n",
       " 53,\n",
       " 1415,\n",
       " 26,\n",
       " 29609]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('crypt7_1Thu00:00O35060.0H35071.34L34985.47C34995.83V20;crypt7_1Thu00:01O34995.83H35049.37L34971.47C34992.57V14;crypt7_1Thu00:02O34988.11H35014.53L34931.08C34961.2V75;crypt7_1Thu00:03O34967.9H35005.65L34954.13C34961.23V27;crypt7_1Thu00:04O34962.6H35010.7L34962.59C34983.38V10;crypt7_1Thu00:05O34971.31H34983.37L34904.51C34905.03V13;crypt7_1Thu00:06O34905.05H34939.0L34900.01C34930.8V19;crypt7_1Thu00:07O34912.38H34976.0L34910.43C34935.3V29;crypt7_1Thu00:08O34936.23H34979.95L34918.74C34979.95V14;crypt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['110']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_new.convert_ids_to_tokens(tokenizer_new.encode(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008, 50257)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_new.vocab_size, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_done_files = [f[:-4] for f in listdir('tokenized_subset_data/train/')]\n",
    "val_done_files = [f[:-4] for f in listdir('tokenized_subset_data/val/')]\n",
    "\n",
    "train_files = [f for f in listdir('strings/train/') if f[:-11] not in train_done_files]\n",
    "val_files = [f for f in listdir('strings/val/') if f[:-11] not in val_done_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_1min\n",
      "ADA_1min_string.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (37372178 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAT_1min_string.txt\n",
      "BCH_1min_string.txt\n",
      "BNT_1min_string.txt\n",
      "BSV_1min_string.txt\n",
      "BTC_1min_string.txt\n",
      "BTG_1min_string.txt\n",
      "CVC_1min_string.txt\n",
      "DAI_1min_string.txt\n",
      "DASH_1min_string.txt\n",
      "DCR_1min_string.txt\n",
      "DOGE_1min_string.txt\n",
      "EOS_1min_string.txt\n",
      "ETC_1min_string.txt\n",
      "ETH_1min_string.txt\n",
      "FUN_1min_string.txt\n",
      "HT_1min_string.txt\n",
      "ICX_1min_string.txt\n",
      "IOST_1min_string.txt\n",
      "KNC_1min_string.txt\n",
      "LINK_1min_string.txt\n",
      "LRC_1min_string.txt\n",
      "LSK_1min_string.txt\n",
      "LTC_1min_string.txt\n",
      "MAID_1min_string.txt\n",
      "MANA_1min_string.txt\n",
      "MKR_1min_string.txt\n",
      "NEO_1min_string.txt\n",
      "OMG_1min_string.txt\n",
      "ONT_1min_string.txt\n",
      "PAX_1min_string.txt\n",
      "QTUM_1min_string.txt\n",
      "REP_1min_string.txt\n",
      "SC_1min_string.txt\n",
      "SNT_1min_string.txt\n",
      "TRX_1min_string.txt\n",
      "UTK_1min_string.txt\n",
      "VET_1min_string.txt\n",
      "WAVES_1min_string.txt\n",
      "XEM_1min_string.txt\n",
      "XLM_1min_string.txt\n",
      "XMR_1min_string.txt\n",
      "XRP_1min_string.txt\n",
      "XTZ_1min_string.txt\n",
      "XVG_1min_string.txt\n",
      "ZEC_1min_string.txt\n",
      "ZIL_1min_string.txt\n",
      "ZRX_1min_string.txt\n",
      "5min\n",
      "ADA_5min_string.txt\n",
      "BAT_5min_string.txt\n",
      "BCH_5min_string.txt\n",
      "BNT_5min_string.txt\n",
      "BSV_5min_string.txt\n",
      "BTC_5min_string.txt\n",
      "BTG_5min_string.txt\n",
      "CVC_5min_string.txt\n",
      "DAI_5min_string.txt\n",
      "DASH_5min_string.txt\n",
      "DCR_5min_string.txt\n",
      "DOGE_5min_string.txt\n",
      "EOS_5min_string.txt\n",
      "ETC_5min_string.txt\n",
      "ETH_5min_string.txt\n",
      "FUN_5min_string.txt\n",
      "HT_5min_string.txt\n",
      "ICX_5min_string.txt\n",
      "IOST_5min_string.txt\n",
      "KNC_5min_string.txt\n",
      "LINK_5min_string.txt\n",
      "LRC_5min_string.txt\n",
      "LSK_5min_string.txt\n",
      "LTC_5min_string.txt\n",
      "MAID_5min_string.txt\n",
      "MANA_5min_string.txt\n",
      "MKR_5min_string.txt\n",
      "NEO_5min_string.txt\n",
      "OMG_5min_string.txt\n",
      "ONT_5min_string.txt\n",
      "PAX_5min_string.txt\n",
      "QTUM_5min_string.txt\n",
      "REP_5min_string.txt\n",
      "SC_5min_string.txt\n",
      "SNT_5min_string.txt\n",
      "TRX_5min_string.txt\n",
      "UTK_5min_string.txt\n",
      "VET_5min_string.txt\n",
      "WAVES_5min_string.txt\n",
      "XEM_5min_string.txt\n",
      "XLM_5min_string.txt\n",
      "XMR_5min_string.txt\n",
      "XRP_5min_string.txt\n",
      "XTZ_5min_string.txt\n",
      "XVG_5min_string.txt\n",
      "ZEC_5min_string.txt\n",
      "ZIL_5min_string.txt\n",
      "ZRX_5min_string.txt\n",
      "_30min\n",
      "ADA_30min_string.txt\n",
      "BAT_30min_string.txt\n",
      "BCH_30min_string.txt\n",
      "BNT_30min_string.txt\n",
      "BSV_30min_string.txt\n",
      "BTC_30min_string.txt\n",
      "BTG_30min_string.txt\n",
      "CVC_30min_string.txt\n",
      "DAI_30min_string.txt\n",
      "DASH_30min_string.txt\n",
      "DCR_30min_string.txt\n",
      "DOGE_30min_string.txt\n",
      "EOS_30min_string.txt\n",
      "ETC_30min_string.txt\n",
      "ETH_30min_string.txt\n",
      "FUN_30min_string.txt\n",
      "HT_30min_string.txt\n",
      "ICX_30min_string.txt\n",
      "IOST_30min_string.txt\n",
      "KNC_30min_string.txt\n",
      "LINK_30min_string.txt\n",
      "LRC_30min_string.txt\n",
      "LSK_30min_string.txt\n",
      "LTC_30min_string.txt\n",
      "MAID_30min_string.txt\n",
      "MANA_30min_string.txt\n",
      "MKR_30min_string.txt\n",
      "NEO_30min_string.txt\n",
      "OMG_30min_string.txt\n",
      "ONT_30min_string.txt\n",
      "PAX_30min_string.txt\n",
      "QTUM_30min_string.txt\n",
      "REP_30min_string.txt\n",
      "SC_30min_string.txt\n",
      "SNT_30min_string.txt\n",
      "TRX_30min_string.txt\n",
      "UTK_30min_string.txt\n",
      "VET_30min_string.txt\n",
      "WAVES_30min_string.txt\n",
      "XEM_30min_string.txt\n",
      "XLM_30min_string.txt\n",
      "XMR_30min_string.txt\n",
      "XRP_30min_string.txt\n",
      "XTZ_30min_string.txt\n",
      "XVG_30min_string.txt\n",
      "ZEC_30min_string.txt\n",
      "ZIL_30min_string.txt\n",
      "ZRX_30min_string.txt\n",
      "_1hour\n",
      "ADA_1hour_string.txt\n",
      "BAT_1hour_string.txt\n",
      "BCH_1hour_string.txt\n",
      "BNT_1hour_string.txt\n",
      "BSV_1hour_string.txt\n",
      "BTC_1hour_string.txt\n",
      "BTG_1hour_string.txt\n",
      "CVC_1hour_string.txt\n",
      "DAI_1hour_string.txt\n",
      "DASH_1hour_string.txt\n",
      "DCR_1hour_string.txt\n",
      "DOGE_1hour_string.txt\n",
      "EOS_1hour_string.txt\n",
      "ETC_1hour_string.txt\n",
      "ETH_1hour_string.txt\n",
      "FUN_1hour_string.txt\n",
      "HT_1hour_string.txt\n",
      "ICX_1hour_string.txt\n",
      "IOST_1hour_string.txt\n",
      "KNC_1hour_string.txt\n",
      "LINK_1hour_string.txt\n",
      "LRC_1hour_string.txt\n",
      "LSK_1hour_string.txt\n",
      "LTC_1hour_string.txt\n",
      "MAID_1hour_string.txt\n",
      "MANA_1hour_string.txt\n",
      "MKR_1hour_string.txt\n",
      "NEO_1hour_string.txt\n",
      "OMG_1hour_string.txt\n",
      "ONT_1hour_string.txt\n",
      "PAX_1hour_string.txt\n",
      "QTUM_1hour_string.txt\n",
      "REP_1hour_string.txt\n",
      "SC_1hour_string.txt\n",
      "SNT_1hour_string.txt\n",
      "TRX_1hour_string.txt\n",
      "UTK_1hour_string.txt\n",
      "VET_1hour_string.txt\n",
      "WAVES_1hour_string.txt\n",
      "XEM_1hour_string.txt\n",
      "XLM_1hour_string.txt\n",
      "XMR_1hour_string.txt\n",
      "XRP_1hour_string.txt\n",
      "XTZ_1hour_string.txt\n",
      "XVG_1hour_string.txt\n",
      "ZEC_1hour_string.txt\n",
      "ZIL_1hour_string.txt\n",
      "ZRX_1hour_string.txt\n",
      "Wall time: 2h 45min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for frame in ['_1min', '5min', '_30min', '_1hour']:\n",
    "    print(frame)\n",
    "    for file in train_files:\n",
    "        if frame in file:\n",
    "            samples_tokenized2 = []\n",
    "            print(file)\n",
    "            f = Path('strings/train/'+file).read_text()\n",
    "            tokens = tokenizer.encode(f)\n",
    "            for i in range(0, len(tokens), 2048):\n",
    "                samples_tokenized2.append(tokens[i:i+2048])\n",
    "            del samples_tokenized2[-1]\n",
    "            \n",
    "            samples_tokenized2 = np.array(samples_tokenized2)\n",
    "            np.save(f'tokenized_subset_data/train/{file[:-11]}', samples_tokenized2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_1min\n",
      "ADA_1min_string.txt\n",
      "BAT_1min_string.txt\n",
      "BCH_1min_string.txt\n",
      "BNT_1min_string.txt\n",
      "BSV_1min_string.txt\n",
      "BTC_1min_string.txt\n",
      "BTG_1min_string.txt\n",
      "CVC_1min_string.txt\n",
      "DAI_1min_string.txt\n",
      "DASH_1min_string.txt\n",
      "DCR_1min_string.txt\n",
      "DOGE_1min_string.txt\n",
      "EOS_1min_string.txt\n",
      "ETC_1min_string.txt\n",
      "ETH_1min_string.txt\n",
      "FUN_1min_string.txt\n",
      "HT_1min_string.txt\n",
      "ICX_1min_string.txt\n",
      "IOST_1min_string.txt\n",
      "KNC_1min_string.txt\n",
      "LINK_1min_string.txt\n",
      "LRC_1min_string.txt\n",
      "LSK_1min_string.txt\n",
      "LTC_1min_string.txt\n",
      "MAID_1min_string.txt\n",
      "MANA_1min_string.txt\n",
      "MKR_1min_string.txt\n",
      "NEO_1min_string.txt\n",
      "OMG_1min_string.txt\n",
      "ONT_1min_string.txt\n",
      "PAX_1min_string.txt\n",
      "QTUM_1min_string.txt\n",
      "REP_1min_string.txt\n",
      "SC_1min_string.txt\n",
      "SNT_1min_string.txt\n",
      "TRX_1min_string.txt\n",
      "UTK_1min_string.txt\n",
      "VET_1min_string.txt\n",
      "WAVES_1min_string.txt\n",
      "XEM_1min_string.txt\n",
      "XLM_1min_string.txt\n",
      "XMR_1min_string.txt\n",
      "XRP_1min_string.txt\n",
      "XTZ_1min_string.txt\n",
      "XVG_1min_string.txt\n",
      "ZEC_1min_string.txt\n",
      "ZIL_1min_string.txt\n",
      "ZRX_1min_string.txt\n",
      "5min\n",
      "ADA_5min_string.txt\n",
      "BAT_5min_string.txt\n",
      "BCH_5min_string.txt\n",
      "BNT_5min_string.txt\n",
      "BSV_5min_string.txt\n",
      "BTC_5min_string.txt\n",
      "BTG_5min_string.txt\n",
      "CVC_5min_string.txt\n",
      "DAI_5min_string.txt\n",
      "DASH_5min_string.txt\n",
      "DCR_5min_string.txt\n",
      "DOGE_5min_string.txt\n",
      "EOS_5min_string.txt\n",
      "ETC_5min_string.txt\n",
      "ETH_5min_string.txt\n",
      "FUN_5min_string.txt\n",
      "HT_5min_string.txt\n",
      "ICX_5min_string.txt\n",
      "IOST_5min_string.txt\n",
      "KNC_5min_string.txt\n",
      "LINK_5min_string.txt\n",
      "LRC_5min_string.txt\n",
      "LSK_5min_string.txt\n",
      "LTC_5min_string.txt\n",
      "MAID_5min_string.txt\n",
      "MANA_5min_string.txt\n",
      "MKR_5min_string.txt\n",
      "NEO_5min_string.txt\n",
      "OMG_5min_string.txt\n",
      "ONT_5min_string.txt\n",
      "PAX_5min_string.txt\n",
      "QTUM_5min_string.txt\n",
      "REP_5min_string.txt\n",
      "SC_5min_string.txt\n",
      "SNT_5min_string.txt\n",
      "TRX_5min_string.txt\n",
      "UTK_5min_string.txt\n",
      "VET_5min_string.txt\n",
      "WAVES_5min_string.txt\n",
      "XEM_5min_string.txt\n",
      "XLM_5min_string.txt\n",
      "XMR_5min_string.txt\n",
      "XRP_5min_string.txt\n",
      "XTZ_5min_string.txt\n",
      "XVG_5min_string.txt\n",
      "ZEC_5min_string.txt\n",
      "ZIL_5min_string.txt\n",
      "ZRX_5min_string.txt\n",
      "_30min\n",
      "ADA_30min_string.txt\n",
      "BAT_30min_string.txt\n",
      "BCH_30min_string.txt\n",
      "BNT_30min_string.txt\n",
      "BSV_30min_string.txt\n",
      "BTC_30min_string.txt\n",
      "BTG_30min_string.txt\n",
      "CVC_30min_string.txt\n",
      "DAI_30min_string.txt\n",
      "DASH_30min_string.txt\n",
      "DCR_30min_string.txt\n",
      "DOGE_30min_string.txt\n",
      "EOS_30min_string.txt\n",
      "ETC_30min_string.txt\n",
      "ETH_30min_string.txt\n",
      "FUN_30min_string.txt\n",
      "HT_30min_string.txt\n",
      "ICX_30min_string.txt\n",
      "IOST_30min_string.txt\n",
      "KNC_30min_string.txt\n",
      "LINK_30min_string.txt\n",
      "LRC_30min_string.txt\n",
      "LSK_30min_string.txt\n",
      "LTC_30min_string.txt\n",
      "MAID_30min_string.txt\n",
      "MANA_30min_string.txt\n",
      "MKR_30min_string.txt\n",
      "NEO_30min_string.txt\n",
      "OMG_30min_string.txt\n",
      "ONT_30min_string.txt\n",
      "PAX_30min_string.txt\n",
      "QTUM_30min_string.txt\n",
      "REP_30min_string.txt\n",
      "SC_30min_string.txt\n",
      "SNT_30min_string.txt\n",
      "TRX_30min_string.txt\n",
      "UTK_30min_string.txt\n",
      "VET_30min_string.txt\n",
      "WAVES_30min_string.txt\n",
      "XEM_30min_string.txt\n",
      "XLM_30min_string.txt\n",
      "XMR_30min_string.txt\n",
      "XRP_30min_string.txt\n",
      "XTZ_30min_string.txt\n",
      "XVG_30min_string.txt\n",
      "ZEC_30min_string.txt\n",
      "ZIL_30min_string.txt\n",
      "ZRX_30min_string.txt\n",
      "_1hour\n",
      "ADA_1hour_string.txt\n",
      "BAT_1hour_string.txt\n",
      "BCH_1hour_string.txt\n",
      "BNT_1hour_string.txt\n",
      "BSV_1hour_string.txt\n",
      "BTC_1hour_string.txt\n",
      "BTG_1hour_string.txt\n",
      "CVC_1hour_string.txt\n",
      "DAI_1hour_string.txt\n",
      "DASH_1hour_string.txt\n",
      "DCR_1hour_string.txt\n",
      "DOGE_1hour_string.txt\n",
      "EOS_1hour_string.txt\n",
      "ETC_1hour_string.txt\n",
      "ETH_1hour_string.txt\n",
      "FUN_1hour_string.txt\n",
      "HT_1hour_string.txt\n",
      "ICX_1hour_string.txt\n",
      "IOST_1hour_string.txt\n",
      "KNC_1hour_string.txt\n",
      "LINK_1hour_string.txt\n",
      "LRC_1hour_string.txt\n",
      "LSK_1hour_string.txt\n",
      "LTC_1hour_string.txt\n",
      "MAID_1hour_string.txt\n",
      "MANA_1hour_string.txt\n",
      "MKR_1hour_string.txt\n",
      "NEO_1hour_string.txt\n",
      "OMG_1hour_string.txt\n",
      "ONT_1hour_string.txt\n",
      "PAX_1hour_string.txt\n",
      "QTUM_1hour_string.txt\n",
      "REP_1hour_string.txt\n",
      "SC_1hour_string.txt\n",
      "SNT_1hour_string.txt\n",
      "TRX_1hour_string.txt\n",
      "UTK_1hour_string.txt\n",
      "VET_1hour_string.txt\n",
      "WAVES_1hour_string.txt\n",
      "XEM_1hour_string.txt\n",
      "XLM_1hour_string.txt\n",
      "XMR_1hour_string.txt\n",
      "XRP_1hour_string.txt\n",
      "XTZ_1hour_string.txt\n",
      "XVG_1hour_string.txt\n",
      "ZEC_1hour_string.txt\n",
      "ZIL_1hour_string.txt\n",
      "ZRX_1hour_string.txt\n"
     ]
    }
   ],
   "source": [
    "for frame in ['_1min', '5min', '_30min', '_1hour']:\n",
    "    print(frame)\n",
    "    for file in val_files:\n",
    "        if frame in file:\n",
    "            samples_tokenized2 = []\n",
    "            print(file)\n",
    "            f = Path('strings/val/'+file).read_text()\n",
    "            tokens = tokenizer.encode(f)\n",
    "            for i in range(0, len(tokens), 2048):\n",
    "                samples_tokenized2.append(tokens[i:i+2048])\n",
    "            if len(samples_tokenized2) > 0:\n",
    "                del samples_tokenized2[-1]\n",
    "            \n",
    "            samples_tokenized2 = np.array(samples_tokenized2)\n",
    "            np.save(f'tokenized_subset_data/val/{file[:-11]}', samples_tokenized2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"\n",
    "    Returns an int64_list from a bool / enum / int / uint.\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def write_to_file(writer, data):\n",
    "    \"\"\"\n",
    "    writes data to tfrecord file\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        \"text\": _int64_feature(data)\n",
    "    }\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "    \n",
    "    \n",
    "def split_list(l, n):\n",
    "    # splits list/string into n size chunks\n",
    "    return [l[i:i + n] for i in range(0, len(l), n)]\n",
    "\n",
    "\n",
    "def write_files(files, files_per, output_dir, out_name, start_no, write_remainder=True, process_no=None):\n",
    "    # writes a list of files to .tfrecords\n",
    "    chunks = split_list(files, files_per)\n",
    "\n",
    "    if len(chunks[-1]) != files_per and not write_remainder:  # pop the last file if it's length != files per\n",
    "        remainder = chunks.pop(-1)\n",
    "    else:\n",
    "        remainder = None  # assuming files = remainder from an old chunk here\n",
    "        files_per = len(chunks[-1])\n",
    "\n",
    "    for files in chunks:\n",
    "        fp = f\"{output_dir}/{out_name}_{start_no}\"\n",
    "        if process_no is not None:\n",
    "            fp += f\"_{process_no}\"\n",
    "        fp += f\"_{files_per}\"  # add number of files in tfrecord to end of fp\n",
    "        fp += \".tfrecords\"\n",
    "        with tf.io.TFRecordWriter(fp) as writer:\n",
    "            for f in files:\n",
    "                write_to_file(writer, f)\n",
    "        start_no += 1\n",
    "    return start_no, remainder\n",
    "\n",
    "\n",
    "def create_tfrecords(tokenized_files_array, output_dir, out_name):\n",
    "    files_per = 100000\n",
    "    _tfrecord_count, remainder = write_files(tokenized_files_array, files_per=files_per,\n",
    "                                                         output_dir=output_dir, out_name=out_name,\n",
    "                                                         start_no=0, process_no=None)\n",
    "    \n",
    "@tf.function\n",
    "def decode_fn(record_bytes):\n",
    "  return tf.io.parse_single_example(\n",
    "      # Data\n",
    "      record_bytes,\n",
    "\n",
    "      # Schema\n",
    "      {\"x\": tf.io.FixedLenFeature([], dtype=tf.float32),\n",
    "       \"y\": tf.io.FixedLenFeature([], dtype=tf.float32)}\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_tokenized3 = np.concatenate([np.load(f'tokenized_subset_data/train/{file[:-11]}.npy') for file in train_files])\n",
    "results = create_tfrecords(samples_tokenized3, 'records_subset', 'crypto_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_tokenized3 = np.concatenate([np.load(f'tokenized_subset_data/val/{file[:-11]}.npy') for file in val_files if np.load(f'tokenized_subset_data/val/{file[:-11]}.npy').shape[0] > 0])\n",
    "results = create_tfrecords(samples_tokenized3, 'records_subset', 'crypto_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GPT2_final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
